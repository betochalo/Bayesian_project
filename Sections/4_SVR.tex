The Support Vector Regression (SVR) algorithm was formulated by Vapnik and Chervonenkis $(1964)$
and is a machine-learning algorithm that is perhaps the most elegant of all kernel-learning 
methods. The SVR consist of a small subset of data points extracted by the learning algorithm
from the training sample itself. SVR models have recently been used to
handle problems such as nonlinear, local minimum, and high dimension. This model can even 
guarantee higher accuracy for long-term predictions compared to other computational approaches
in many applications.\\
 Suppose the training data given are $\mathcal{D}=\left\{(x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\}\subset\mathcal{X}$
where $x_{i}$ is the input pattern for the $i-$th example $x_{i}\in \mathcal{X}= \mathbb{R}^{n}$,
and $y_{i}=f(x_{i})\in  \mathbb{R}$ is the corresponding desired response (target output),
Haykin $(2009)$, Muthukrishnan and Maryam $(2020)$, K Abdul Hamid et. al  $(2023)$.
The function$f:\mathcal{X}\rightarrow \mathbb{R}$ can be described as
\begin{align}
x\rightarrow f(x):=\langle w,x\rangle+b=w^{T}x + b,\quad w\in \mathcal{X},
\quad b\in \mathbb{R} \label{Equa:hyperplane}
\end{align}
 where $\langle \cdot,\cdot\rangle$ denotes the dot product in $\mathcal{X}$,
$x$ is an input vector, $w$ is an adjustable weight vector, and $b$ is a bias.\\
Now we define the optimization problem. Given the training sample $\mathcal{D}$, 
we want find the optimum values of the weight  vector $w$ and bias $b$ such that they
 satisfy the constraints
\begin{align}
y_{i}\left(w^{T}x_{i}+b\right) \geq 1,\quad i=1,\ldots,n
\end{align}
and the weight vector $w$ minimizes the cost function
 \begin{align}
	\min\frac{1}{2}\|w\|^{2} =\min\frac{1}{2}w^{T}w\label{Equa:convex}
\end{align}
We construct the Lagrangian function, Bertsekas, $(1995)$
\begin{align}
J(w,b,\alpha)=\frac{1}{2}w^{T}w-\sum_{i=1}^{n}\alpha_{i}
\left[y_{i}\left(w^{T}x_{i}+b\right) -1 \right] \label{Equa:conditions1}
\end{align}
where the auxiliary nonnegative variables $\alpha_{i}$ are called Lagrange multipliers.
The solution to the constrained-optimization problem is determined by the saddle point
of the Lagrangian function $J(w,b,\alpha)$, i.e
\begin{align}
\frac{\partial J(w,b,\alpha)}{\partial w}=0,\quad \frac{\partial J(w,b,\alpha)}{\partial b}=0
\label{Equa:conditions2}
\end{align}
Application (\ref{Equa:conditions1}) and (\ref{Equa:conditions2}) yields
the following:
 \begin{align}
 w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i},\quad\textit{and}\quad \sum_{i=1}^{n}\alpha_{i}y_{i}=0
 \end{align}
 The solution vector $w$ is defined in terms of an expansion that involves the $n$ training
 examples.\\
 The SVR can be viewed as a kernel machine inner product kernel.
 Let $x$ denote a vector drawn from the input space of dimension $m$. Let 
 $\{\varphi_{i}(x)\}_{i=1}^{\infty}$ denote  a set of nonlinear functions that, 
 between them, transform the input space of dimension $m$ to a feature space of
  infinite dimensionality. Given this transformation, we may define
 a hyperplane acting as the decision surface in accordance with the formula
 \begin{align*}
 \sum_{i=1}^{\infty}w_{i}\varphi_{i}(x)=0
 \end{align*}
 where  $\{w_{i}\}_{i=1}^{\infty}$ denotes an infinitely 
 large set of  weights that transforms the feature space  to the output space.
 Using matrix notation:
  \begin{align}
 w^{T}\Phi(x)=0
  \end{align}
  where $\Phi(x)$ is the feature vector and $w$ is the corresponding weight vector.
 Now we may expressing the weight vector as
   \begin{align}
  w=\sum_{i=1}^{n}\alpha_{i}y_{i}\Phi(x_{i})
  \end{align}
  where the feature vector is expressed as
   \begin{align}
   	\Phi(x_{i})=\left(\varphi_{1}(x_{i}),\varphi_{2}(x_{i}),\ldots \right)^{T}
\end{align}
 Then we may express the decision surface in the output space as
 \begin{align*}
 \sum_{i=1}^{n}\alpha_{i}y_{i}\Phi(x_{i})^{T}\Phi(x)=0
 \end{align*}
 where $n$ is the number of support vectors and $\Phi(x_{i})^{T}\Phi(x)$ represents an
 inner product, where this inner-product term be denoted as
 \begin{align}
 k\left(x,x_{i}\right)= \Phi(x_{i})^{T}\Phi(x)=\sum_{j=1}^{\infty}\varphi_{j}(x_{i})\varphi_{j}(x)
 \end{align}
 The optimal decision surface in the output space as
 \begin{align}
 \sum_{i=1}^{n}\alpha_{i}y_{i} k\left(x,x_{i}\right)=0
 \end{align}
 The kernel $ k\left(x,x_{i}\right)$ is a function that computes the inner product of the images produced in
 the feature space under the embedding $\Phi$ of two data points in the input space, Shawe-Taylor and 
 Cristianini, $(2004)$. \\
 A covariance function , also called a kernel, kernel function, or
 covariance kernel, is a positive-definite function of two inputs
 $x$ and $x^{\prime}$. When we select a specific covariance function,
 we select if our solution should be smooth,
 linear, periodic and polynomial.\\
 For a given training dataset $\mathcal{D}$,
 the covariance function generates the covariance matrix, $ \mathcal{K}\left(x,x^{\prime}\right)$
 and this describes the correlation between different points in the process.
 \begin{align}
 	\mathbb{C}ov\left(f(x),f(x^{\prime}) \right) =
 \mathcal{K}\left(x,x^{\prime}\right)=
 	\begin{pmatrix}
 	\mathbb{C}ov\left(x_{1},x_{1}\right)  & \ldots & \mathbb{C}ov\left(x_{1},x_{n}\right)\\
 	\mathbb{C}ov\left(x_{2},x_{1}\right)  & \ldots & \mathbb{C}ov\left(x_{2},x_{n}\right)\\
 		\vdots &  \vdots & \vdots & \\
 		\mathbb{C}ov\left(x_{n},x_{1}\right) &\ldots&\mathbb{C}ov\left(x_{n},x_{n}\right)
 	\end{pmatrix}
 \end{align}
 where $\mathbb{C}ov\left(f(x_{i}),f(x_{j}) \right)=k\left(x_{i},x_{j}\right)$.\\
 We summarize the kernels for three common types of support vector machines:
 polynomial learning machine, radial-basis-function network, and two-layer perceptron
 \begin{enumerate}
 \item Polynomial learning machine kernel:
 \begin{align*}
 k\left(x,x_{i}\right)=\left(x^{T}x_{i} +1\right)^{p},\quad i=1,\ldots,n
 \end{align*}
 power $p$ is specified a prior.
 \item Radial-basis-function network kernel:
 \begin{align*}
 k\left(x,x_{i}\right)= \exp\left(-\frac{1}{2\sigma^{2}}\|x-x_{i}\|^{2}\right),\quad i=1,\ldots,n
 \end{align*}
 The width $\sigma^{2}$, common to all the kernels, is specified a prior.
\item Two-layer perceptron:
 \begin{align*}
 k\left(x,x_{i}\right)=\tanh\left(\beta_{0}x^{T}x_{i}+\beta_{1} \right),\quad i=1,\ldots,n
\end{align*}
\item A nonstationary neural network kernel:
\begin{align*}
	k\left(x_{i},x_{j}\right)=\frac{2}{\pi}\sin^{-1}
	\left(\frac{2\hat{x}_{i}^{T}\Sigma_{*}\hat{x}_{j}^{T}}
	{\left( 1+2\hat{x}_{i}^{T}\Sigma_{*}\hat{x}_{i}^{T}\right) 
		\left(1+2\hat{x}_{j}^{T}\Sigma_{*}\hat{x}_{j}^{T} \right) } \right) 
\end{align*}
where $\hat{x}=(1,x_{1},\ldots,x_{n})$ is the input vector and 
$\Sigma_{*}=diag\left(\sigma_{0}^{2},\sigma_{1}^{2},\ldots,\sigma_{n}^{2} \right)$
is a diagonal weight prior, $\sigma_{0}^{2}$ is a variance for bias parameter.
\item Periodic kernel:
\begin{align*}
	k\left(x_{i},x_{j}\right)=\sigma_{p}\exp\left\{-\frac{2}{l^{2}}\sin^{2}
	\left[\pi\left(\frac{x_{i}-x_{j}}{p} \right)\right]\right\}  
\end{align*}
where $l$ is a parameter that controls the smoothness of the function
and $p$ governs the inverse length of the periodicity.
 \end{enumerate}
 For the symmetric kernel $k\left(x,x_{i}\right)$ is an special case of Mercerâ€™s theorem,
 Mercer, $(1909)$; Courant and Hilbert, $(1970)$.
 Let $k(x,x^{\prime})$ be a continuous symmetric kernel that is defined in the closed interval
 $a\leq x\leq b$. The kernel $k(x,x^{\prime})$ can be expanded in the series:
 \begin{align*}
 k\left(x,x^{\prime}\right)=\sum_{i=1}^{\infty}\lambda_{i}\varphi_{i}(x)\varphi_{i}(x^{\prime})
 \end{align*}
 with positive coefficients  $\lambda_{i} >0$ for all $i$. For this expansion to be valid and 
 for it to converge absolutely and uniformly, it is necessary and sufficient that the condition
  \begin{align*}
  \int_{b}^{a}\int_{b}^{a} k\left(x,x^{\prime}\right)\psi(x)\psi(x^{\prime})dxdx^{\prime}\geq 0
 \end{align*}
 holds for all $\psi(.)$, for which we have $\int_{b}^{a}\psi^{2}(x)dx<\infty$.
 where $a$ and $b$ are the constants of integretion.
 The features $\varphi_{i}(x)$ are called eigenfunctions of the expansion, and the numbers 
 $\lambda_{i}$ are called eigenvalues, Haykin $(2009)$.
